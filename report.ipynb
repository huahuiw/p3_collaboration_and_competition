{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\", no_graphics = False)\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build and train the model\n",
    "\n",
    "A multi-agent DDPG algorithm is used in this implementation. Two agents each having an actor-critic neural network will share the replay buffer and learn from joint experiences. Both actor and critic networks have local and target networks to improve the training. Specifically, two hidden layers of (512, 256) are used for the actor networks and (512, 256) for the critic networks.The critic networks use joint observations and joint actions from both agents and the actor networks use only local observations from individual agents. The learning rate for the actor and critic networks are LR_ACTOR = 1e-4 and LR_CRITIC = 1e-3, respectively. \n",
    "\n",
    "At the beginning of the training, we increase the exploration of the agent by adding a Gaussian noise to the actions, with an original variance of 0.5. But the variance of the noise keeps reducing till 0.1 during the training process. A decaying factor of 0.999 is applied to the variance after each episode. \n",
    "\n",
    "Other details:\n",
    "Soft update is performed for the target network with a Tau value of 0.001.\n",
    "The buffer size for the replay buffer is set to be 1e6. The training is performed in batches of 256 experiences. The rewards are discounted with gamma = 0.99.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 50, average score: 0.017600000277161597, noise_sigma: 0.47560281409851574\n",
      "episode 100, average score: 0.0157000002451241, noise_sigma: 0.4523960735568548\n",
      "episode 150, average score: 0.014600000232458115, noise_sigma: 0.43032169134151843\n",
      "episode 200, average score: 0.009700000155717134, noise_sigma: 0.409324414739318\n",
      "episode 250, average score: 0.0020000000298023225, noise_sigma: 0.3893516870584952\n",
      "episode 300, average score: 0.0010000000149011613, noise_sigma: 0.37035351607804984\n",
      "episode 350, average score: 0.0010000000149011613, noise_sigma: 0.3522823489160005\n",
      "episode 400, average score: 0.006500000115484, noise_sigma: 0.33509295300337016\n",
      "episode 450, average score: 0.00830000014975667, noise_sigma: 0.3187423028659689\n",
      "episode 500, average score: 0.002800000049173832, noise_sigma: 0.3031894724305924\n",
      "episode 550, average score: 0.0020000000298023225, noise_sigma: 0.2883955325860681\n",
      "episode 600, average score: 0.004000000059604645, noise_sigma: 0.27432345374274825\n",
      "episode 650, average score: 0.0030000000447034836, noise_sigma: 0.26093801314655013\n",
      "episode 700, average score: 0.0009000000171363354, noise_sigma: 0.24820570671554945\n",
      "episode 750, average score: 0.005600000098347664, noise_sigma: 0.23609466517845223\n",
      "episode 800, average score: 0.017200000286102295, noise_sigma: 0.2245745743050374\n",
      "episode 850, average score: 0.02770000046119094, noise_sigma: 0.213616599028904\n",
      "episode 900, average score: 0.02870000049471855, noise_sigma: 0.20319331127260196\n",
      "episode 950, average score: 0.02380000041797757, noise_sigma: 0.19327862129449025\n",
      "episode 1000, average score: 0.022100000362843274, noise_sigma: 0.18384771238548175\n",
      "episode 1050, average score: 0.051500000786036254, noise_sigma: 0.1748769787522195\n",
      "episode 1100, average score: 0.06730000102892518, noise_sigma: 0.16634396643120383\n",
      "episode 1150, average score: 0.046100000757724045, noise_sigma: 0.15822731708597912\n",
      "episode 1200, average score: 0.04020000074058771, noise_sigma: 0.1505067145466996\n",
      "episode 1250, average score: 0.04420000083744526, noise_sigma: 0.14316283395826473\n",
      "episode 1300, average score: 0.04080000076442957, noise_sigma: 0.13617729340973853\n",
      "episode 1350, average score: 0.041600000746548176, noise_sigma: 0.12953260792398183\n",
      "episode 1400, average score: 0.05150000089779496, noise_sigma: 0.12321214569233088\n",
      "episode 1450, average score: 0.054800000935792924, noise_sigma: 0.11720008644477772\n",
      "episode 1500, average score: 0.08630000134930015, noise_sigma: 0.11148138185145114\n",
      "episode 1550, average score: 0.12940000196918844, noise_sigma: 0.10604171785628266\n",
      "episode 1600, average score: 0.1227000018581748, noise_sigma: 0.10086747884857773\n",
      "episode 1650, average score: 0.11200000166893005, noise_sigma: 0.1\n",
      "episode 1700, average score: 0.12300000183284282, noise_sigma: 0.1\n",
      "episode 1750, average score: 0.1310000019520521, noise_sigma: 0.1\n",
      "episode 1800, average score: 0.1309000019542873, noise_sigma: 0.1\n",
      "episode 1850, average score: 0.12490000186488032, noise_sigma: 0.1\n",
      "episode 1900, average score: 0.1310000019520521, noise_sigma: 0.1\n",
      "episode 1950, average score: 0.12590000187978148, noise_sigma: 0.1\n",
      "episode 2000, average score: 0.14830000225454568, noise_sigma: 0.1\n",
      "episode 2050, average score: 0.22040000332519413, noise_sigma: 0.1\n",
      "episode 2100, average score: 0.24250000363215804, noise_sigma: 0.1\n",
      "episode 2150, average score: 0.2137000032328069, noise_sigma: 0.1\n",
      "episode 2200, average score: 0.2393000036291778, noise_sigma: 0.1\n",
      "episode 2250, average score: 0.4099000061862171, noise_sigma: 0.1\n",
      "episode 2300, average score: 0.48530000733211637, noise_sigma: 0.1\n",
      "episode 2350, average score: 0.5416000081785023, noise_sigma: 0.1\n"
     ]
    }
   ],
   "source": [
    "TAU = 0.001\n",
    "GAMMA = 0.99\n",
    "LR_ACTOR = 1e-4\n",
    "LR_CRITIC = 1e-3\n",
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "noise_sigma = 0.5\n",
    "noise_reduction = 0.999\n",
    "noise_min = 0.1\n",
    "\n",
    "n_episodes = 10000\n",
    "tmax = 1000\n",
    "\n",
    "\n",
    "from maddpg import MADDPG\n",
    "from collections import deque\n",
    "\n",
    "# number of agents \n",
    "num_agents = 2\n",
    "\n",
    "MA_agent = MADDPG(num_agents, state_size=24, action_size=2, buffer_size = BUFFER_SIZE, \\\n",
    "                  batch_size = BATCH_SIZE, lr_actor = LR_ACTOR, lr_critic = LR_CRITIC, sigma = noise_sigma)\n",
    "\n",
    "score_history= []\n",
    "score_window = deque(maxlen = 100)\n",
    "\n",
    "for i in range(1, n_episodes+1):\n",
    "    #print(i)\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    for t in range(tmax):\n",
    "        #print(t)\n",
    "        actions = MA_agent.act(states, training=True)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        MA_agent.step(states, actions, rewards, next_states, dones)\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    noise_sigma = max(noise_sigma*noise_reduction, noise_min)        \n",
    "    MA_agent.update_noise(noise_sigma)\n",
    "        \n",
    "    score_window.append(np.max(scores))\n",
    "    score_history.append(np.max(scores))\n",
    "    #print(i)\n",
    "    if (i%50)==0:\n",
    "        print('episode {}, average score: {}, noise_sigma: {}'.format(i, np.mean(score_window), noise_sigma))\n",
    "    if np.mean(score_window)>0.6:\n",
    "        MA_agent.save_weights()\n",
    "        break\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'score')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(score_history)\n",
    "plt.xlabel('# of episodes')\n",
    "plt.ylabel('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 2.7000000402331352\n"
     ]
    }
   ],
   "source": [
    "from maddpg import MADDPG\n",
    "\n",
    "agent_test = MADDPG(num_agents=2,state_size=24, action_size=2, sigma=0)\n",
    "agent_test.load_weights()\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "scores = np.zeros(agent_test.num_agents)\n",
    "\n",
    "while True:\n",
    "    actions = agent_test.act(states,training=False)        # select an action\\n\",\n",
    "    env_info = env.step(actions)[brain_name]        # send the action to the environment\\n\",\n",
    "    next_states = env_info.vector_observations   # get the next state\\n\",\n",
    "    rewards = env_info.rewards                   # get the reward\\n\",\n",
    "    dones = env_info.local_done                  # see if episode has finished\\n\",\n",
    "    scores += rewards                                # update the score\\n\",\n",
    "    states = next_states                             # roll over the state to next time step\\n\",\n",
    "    if np.any(dones):                                       # exit loop if episode finished\\n\",\n",
    "        break\n",
    "print(\"Score: {}\".format(np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work\n",
    "\n",
    "Although multiple hyperparameters have been tried to improve the speed of convengence as well as the stability of the agents, including changing the structures of the actor/critic networks, learning rates, noise reductions, etc, the agents seem to learn very slowly. In this implementation, two agents perform local observations and execute locally. Two agents share the same replay buffer and use the joint observations/actions in their cirtic networks. As next steps, the following tricks can be tried, such as using prioritized replay experience, adding noise to the parameters instead of to the actions, using dropout in the critic networks, ect. Also, instead of using local critic networks, a centralized critic network can be tried to see if that will improve the training speed and stability more significantly. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
